{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "according-necklace",
   "metadata": {},
   "source": [
    "# Chunking and chinking\n",
    "\n",
    "This notebook explores (briefly) the differences between **chunking** and **chinking** using NLTK.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-employment",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this notebook, we'll use extracts from [*The Gruffalo*](https://en.wikipedia.org/wiki/The_Gruffalo), for no especial reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "referenced-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is this creature with terrible claws and terrible teeth in his terrible jaws?\n"
     ]
    }
   ],
   "source": [
    "text = \"Who is this creature with terrible claws and terrible teeth in his terrible jaws?\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-profile",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "In order to effectively chunk text, we need to process it first. Chunkers expect tagged & tokenised text as inputs, meaning that our text should be split into individual words (tokens) and each token should have an associated **part of speech**.\n",
    "\n",
    "We'll work through the process step-by-step for this first sentence, showing how the text is transformed. Further in the notebook, we'll process text all at once, with `pos_tag(word_tokenize(text))`, but it will still be the same transformations, just in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deadly-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'this', 'creature', 'with', 'terrible', 'claws', 'and', 'terrible', 'teeth', 'in', 'his', 'terrible', 'jaws', '?']\n"
     ]
    }
   ],
   "source": [
    "# Split the text into tokens\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "municipal-battery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Who', 'WP'), ('is', 'VBZ'), ('this', 'DT'), ('creature', 'NN'), ('with', 'IN'), ('terrible', 'JJ'), ('claws', 'NNS'), ('and', 'CC'), ('terrible', 'JJ'), ('teeth', 'NNS'), ('in', 'IN'), ('his', 'PRP$'), ('terrible', 'JJ'), ('jaws', 'NNS'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Add part-of-speech tags to the tokens\n",
    "\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-letters",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking lets us extract related sequences of tokens together as a single unit.\n",
    "\n",
    "\"He has purple prickles all over his back\".\n",
    "\n",
    "In the above sentence, \"purple\" and \"prickles\" form a **noun phrase**: it makes sense to keep them together as a single unit, rather than view them as unrelated. In the next few examples, we'll use NLTK to extract noun phrases from our text data.\n",
    "\n",
    "### Defining a grammar\n",
    "\n",
    "In order to extract chunks, we need to define a **grammar**: a computer-readable description of what the chunks we're interested in look like. A grammar defines a specific chunk by detailing the tokens within a chunk using (a version of) regex syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparative-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_grammar = \"NP: {<DT>?<JJ>*<NNS?>}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-twelve",
   "metadata": {},
   "source": [
    "The `np_grammar` variable describes one possible noun phrase chunk definition. The table below shows what each sequence of characters in the grammar means.\n",
    "\n",
    "| Sequence | Meaning |\n",
    "| --- | --- |\n",
    "| `NP:` | The name of a chunk, in this case \"NP\" |\n",
    "| `{` | Start of chunk |\n",
    "| `<DT>` | A **determiner**, such as \"a\" or \"the\" |\n",
    "| `?` | 0 or 1 of the preceding token |\n",
    "| `<JJ>` | An **adjective**, like \"purple\" in \"purple prickles\" |\n",
    "| `*` | 0 or more of the preceding token |\n",
    "| `<NNS?>`| A **noun** (singular or plural), like \"prickles\" in \"purple prickles\" |\n",
    "| `}` | End of chunk |\n",
    "\n",
    "In more human-accessible language, the grammar defines a chunk called \"NP\". `NP` chunks consist of an (optional) determiner, 0 or more adjectives, and a final noun.\n",
    "\n",
    "This grammar would find \"purple prickles\", but also \"sharp purple prickles\", \"the prickles\", \"those purple prickles\" and so on.\n",
    "\n",
    "### Using a parser\n",
    "\n",
    "Once we've defined a grammar, we next need to get something that can chunk tokens *using* that grammar. For that, we'll create a **regex parser** using NLTK. This will be able to read through our tokens, combining sequences that match the `NP` grammar into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defensive-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parser that can read the grammar\n",
    "\n",
    "np_parser = RegexpParser(np_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collaborative-honey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Who/WP\n",
      "  is/VBZ\n",
      "  (NP this/DT creature/NN)\n",
      "  with/IN\n",
      "  (NP terrible/JJ claws/NNS)\n",
      "  and/CC\n",
      "  (NP terrible/JJ teeth/NNS)\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  (NP terrible/JJ jaws/NNS)\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "# Actually parse the tokens\n",
    "\n",
    "parsed_tokens = np_parser.parse(tagged_tokens)\n",
    "\n",
    "print(parsed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-sphere",
   "metadata": {},
   "source": [
    "The parser has wrapped everything in a top-level `S` chunk. Within that, there are many independent tokens, but also four `NP` chunks. One chunk is formed by \"this\" (a determiner) and \"creature\" (a singular noun), while the others - still valid `NP` chunks - each consist of an adjective and a plural noun.\n",
    "\n",
    "We can use the same grammar and parser to get the `NP` chunks in another piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suspected-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text\n",
    "\n",
    "text = \"'Here, by these rocks, and his favourite food is… roasted fox!' the mouse answered.\"\n",
    "\n",
    "tagged_tokens = pos_tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "emerging-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  'Here/RB\n",
      "  ,/,\n",
      "  by/IN\n",
      "  (NP these/DT rocks/NNS)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  (NP favourite/JJ food/NN)\n",
      "  (NP is…/NN)\n",
      "  roasted/VBD\n",
      "  fox/RB\n",
      "  !/.\n",
      "  '/''\n",
      "  (NP the/DT mouse/NN)\n",
      "  answered/VBD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Chunk it\n",
    "\n",
    "parsed_tokens = np_parser.parse(tagged_tokens)\n",
    "\n",
    "print(parsed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-rebecca",
   "metadata": {},
   "source": [
    "Again, the parser has identified several `NP` chunks. Note though, that \"roasted\" and \"fox\" has not been found - our current grammar isn't sophisticated enough to pick up all the combinations we *might* be interested in, just the ones that match our precise specifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-suicide",
   "metadata": {},
   "source": [
    "## Chinking\n",
    "\n",
    "Chinking is a way to simplify your chunks down; once you've defined a chunk that groups a sequence of tokens, you can then use a chink to snip out some of the chunk.\n",
    "\n",
    "To demonstrate this, we'll need to use a grammar with more complicated rules. We'll start by defining the basic rule, and then we'll build from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quality-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grammar\n",
    "\n",
    "first_rule_grammar = \"\"\"\n",
    "    ACTOR: {<DT>?<NN><VBD><DT>?<NN>}\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser that can read the grammar\n",
    "\n",
    "fr_parser = RegexpParser(first_rule_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-trial",
   "metadata": {},
   "source": [
    "The `ACTOR` chunk defined above looks for sequences of tokens where something does something to something else. A dog bites a man, for example, or a flower loses a leaf. The table below breaks down the pieces of the pattern.\n",
    "\n",
    "| Sequence | Meaning |\n",
    "| --- | --- |\n",
    "| `ACTOR:` | The name of a chunk, in this case \"ACTOR\" |\n",
    "| `{` | Start of chunk |\n",
    "| `<DT>` | A **determiner**, such as \"a\" or \"the\" |\n",
    "| `?` | 0 or 1 of the preceding token |\n",
    "| `<NN>` | A **noun**, like \"toes\" in \"turned-out toes\" |\n",
    "| `<VBD>` | A past-tense **verb**, such as \"slid\" or \"sped\" |\n",
    "| `}` | End of chunk |\n",
    "\n",
    "In this case, some pieces repeat in the pattern: there is an optional determiner followed by a noun at both the start and the end of the sentence. In between the two is a `VBD` - a past-tense verb. This pattern will match sequences such as \"the mouse found a nut\", or \"woman rides horse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perfect-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some text\n",
    "\n",
    "text = \"A mouse took a stroll through the deep, dark wood. A fox saw the mouse, and the mouse looked good.\"\n",
    "tagged_tokens = pos_tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (ACTOR A/DT mouse/NN took/VBD a/DT stroll/NN)\n",
      "  through/IN\n",
      "  the/DT\n",
      "  deep/JJ\n",
      "  ,/,\n",
      "  dark/JJ\n",
      "  wood/NN\n",
      "  ./.\n",
      "  (ACTOR A/DT fox/NN saw/VBD the/DT mouse/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  the/DT\n",
      "  mouse/NN\n",
      "  looked/VBD\n",
      "  good/JJ\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Parse the text\n",
    "\n",
    "parsed_tokens = fr_parser.parse(tagged_tokens)\n",
    "\n",
    "print(parsed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-premium",
   "metadata": {},
   "source": [
    "In the output above, you can see that two `ACTOR` tokens have been identified: \"a mouse took a stroll\" and \"a fox saw the mouse\". Using just our first rule, we've pulled out phrases following a particular pattern.\n",
    "\n",
    "But we don't have to stop there! The pattern is called `ACTOR`, but at the moment it's pulling out entire actions. What if we wanted to get just the actors from each action: the things being affected or affecting others?\n",
    "\n",
    "To do this, we can use a chink, attaching it onto our `ACTION` rule in the grammar. The combined grammar will follow a two-stage process: first it will chunk actions together, and then it will split each such chunk into two by snipping out the verb and any determiners. The end result will be two `ACTOR` chunks for each action, each one containing a single noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "japanese-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grammar\n",
    "\n",
    "actor_grammar = \"\"\"\n",
    "    ACTOR: {<DT>?<NN><VBD><DT>?<NN>}\n",
    "           }<DT|VBD>{\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser that can read the grammar\n",
    "\n",
    "actor_parser = RegexpParser(actor_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-patrol",
   "metadata": {},
   "source": [
    "The second line - `}<DT|VBD>{` is our chink; note the use of reversed brackets to mark it.\n",
    "\n",
    "| Sequence | Meaning |\n",
    "| --- | --- |\n",
    "| `}` | Start of chink |\n",
    "| `<DT\\|VBD>` | A **determiner** or a past-tense **verb** |\n",
    "| `{` | End of chunk |\n",
    "\n",
    "When used to parse tokens, this will neatly cut out the non-noun bits of our initial `ACTOR` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proud-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/DT\n",
      "  (ACTOR mouse/NN)\n",
      "  took/VBD\n",
      "  a/DT\n",
      "  (ACTOR stroll/NN)\n",
      "  through/IN\n",
      "  the/DT\n",
      "  deep/JJ\n",
      "  ,/,\n",
      "  dark/JJ\n",
      "  wood/NN\n",
      "  ./.\n",
      "  A/DT\n",
      "  (ACTOR fox/NN)\n",
      "  saw/VBD\n",
      "  the/DT\n",
      "  (ACTOR mouse/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  the/DT\n",
      "  mouse/NN\n",
      "  looked/VBD\n",
      "  good/JJ\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Parse the text with the new parser\n",
    "\n",
    "parsed_tokens = actor_parser.parse(tagged_tokens)\n",
    "\n",
    "print(parsed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-junior",
   "metadata": {},
   "source": [
    "Instead of getting the whole action, we now get two `ACTOR` tags for each action, with the determiners and verbs snipped out. Chunking let us focus on just one sequence, and then chinking let us pull out just the bit we want to focus on.\n",
    "\n",
    "As before, this isn't perfect: we might have wanted to pull in \"the mouse looked good\" too, but our pattern wasn't precise enough. We might want not to label \"stroll\" as an actor, because it's not really a thing in the same way as \"fox\" and \"mouse\".\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Text data is difficult to work with because language is complex (and beautiful). You're always going to have edge cases, and always going to need to refine your code to catch those edge cases. With careful application of both chunking and chinking though, you can devise grammars that will let you extract out any data you desire.\n",
    "\n",
    "Personally, I think this is really cool. It can also be useful though too; off the top of my head, here are three potentially interesting/useful applications of chunking & chinking:\n",
    "\n",
    "- Determining who the most active and passive characters are in a narrative\n",
    "- Identifying all the adjectives customers use to refer to a specific product\n",
    "- Highlighting overly-complicated sentence structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
