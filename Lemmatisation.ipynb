{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation with nltk\n",
    "\n",
    "\"Lemmatisation\" is the process of reducing a word down to its \"lemma\" or dictionary form. The words \"walked\", \"walking\", and \"walks\" all have the same lemma - \"walk\", and this is what you would find in a dictionary if you were searching for any of those words. Using lemmatisation, the word \"horses\" is converted to \"horse\", while \"is\", \"are\", \"be\", and \"was\" are all reduced to the root verb \"be\".\n",
    "\n",
    "Lemmatisation is useful for text analysis because it means that words with essentially the same meaning can be considered together, rather than being viewed as unrelated by the computer: \"dog\" and \"dogs\" do not refer to completely separate concepts, and it is important for accurate text analytics that the link is acknowledged.\n",
    "\n",
    "This notebook contains commented examples of how to apply the `nltk`'s `WordNetLemmatizer` to strings. The majority of examples demonstrate how to lemmatise a single string, with the final section demonstrating how to apply the technique to an entire column in a dataframe.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Data manipulation\n",
    "import nltk  # Text analysis methods\n",
    "from nltk.tokenize import word_tokenize  # Split strings into tokens/words\n",
    "from nltk.stem import WordNetLemmatizer  # Reduce a single word to its lemma (root form)\n",
    "from nltk import pos_tag  # Tag a word with its part of speech (e.g. verb)\n",
    "from nltk.corpus import wordnet  # Group words together\n",
    "from collections import defaultdict  # Dictionaries with a fallback value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install nltk wordlists\n",
    "\n",
    "You will need to run `nltk.download(\"all\")` once to download the necessary wordlists that `nltk` depends on. If you'd rather download less, it is also possible to download each list separately.\n",
    "\n",
    "### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_a = \"She runs faster through the shallows to the sandiest banks\"\n",
    "\n",
    "example_b = \"He thanks the women for the happier years\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation\n",
    "\n",
    "The lemmatisation function works on one word at a time - it can't handle strings of multiple words, and just returns them unaltered. In order to lemmatise a string effectively, it must first be split into word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'runs', 'faster', 'through', 'the', 'shallows', 'to', 'the', 'sandiest', 'banks']\n",
      "['He', 'thanks', 'the', 'women', 'for', 'the', 'happier', 'years']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each string, converting them to lists of words\n",
    "\n",
    "example_a_tokens = word_tokenize(example_a)\n",
    "example_b_tokens = word_tokenize(example_b)\n",
    "\n",
    "# Display the tokenized strings\n",
    "\n",
    "print(example_a_tokens)\n",
    "print(example_b_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object that can be used to lemmatise\n",
    "\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'run', 'faster', 'through', 'the', 'shallow', 'to', 'the', 'sandiest', 'bank']\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatisation to each word/token in example_a\n",
    "\n",
    "example_a_lemmas = [lemma.lemmatize(token) for token in example_a_tokens]\n",
    "\n",
    "print(example_a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'thanks', 'the', 'woman', 'for', 'the', 'happier', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatisation to each word/token in example_b\n",
    "\n",
    "example_b_lemmas = [lemma.lemmatize(token) for token in example_b_tokens]\n",
    "\n",
    "print(example_b_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the lemmatiser has had some success - \"year\" has been reduced to \"year\", and \"women\" to \"woman\", for example. However, \"thanks\" is still plural, and \"faster\" has not been converted to \"fast\". \n",
    "\n",
    "This is because, by default, the lemmatiser only works on nouns. Any word passed into the `lemmatize` function is reduced only if the lemmatiser recognises the word as a noun. This means that many words are not reduced even though they do have a base form - unfamiliar nouns and all other parts of speech remain unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Part of speech' tagging\n",
    "\n",
    "\n",
    "In order to lemmatise more effectively, we first need a way of identifying what part of speech (PoS) a particular word is. While this could be done manually, that would be impractical for large amounts of text and tedious even for small ones. The solution is to use another function in `nltk` - `pos_tag`. This function attempts to classify a given word as a particular PoS. It's not a perfect method, but it gets us closer to a more versatile lemmatisation process.\n",
    "\n",
    "The function `pos_tag` takes a set of tokens; it needs the set because one of the ways that the tagger identifies a word is through the words around it: any word after \"very\" is likely to be an adjective, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('runs', 'VBZ'),\n",
       " ('faster', 'RBR'),\n",
       " ('through', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('shallows', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('sandiest', 'JJS'),\n",
       " ('banks', 'NNS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_tag applied to a set of tokens.\n",
    "\n",
    "pos_tag(example_a_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word token is now a `tuple` containing the original word and the `pos_tag` tag.\n",
    "\n",
    "The default tagset - the \"Penn Treebank\" tagset - has [36 different tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), classifying words into highly-specific categories. In the example above, the word \"She\" was tagged as \"PRP\" for \"personal pronoun\", and the word \"sandiest\" was tagged as an \"adjective, superlative\".\n",
    "\n",
    "The WordNetLemmatizer also understands tags, but unfortunately, not the same ones. It's necessary to convert between the two tagging systems. WordNet groups words into four categories: nouns, adjectives, verbs, and adverbs. \n",
    "\n",
    "By converting between the two systems, moving from 36 tags to 4, we will lose some information. Certain words will not be correctly classified, and so will not be converted - the lemmatiser can only convert words it understands. However, this will still result in an improvement upon our previous system of only reducing nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map tags to ones that the lemmatiser will understand.\n",
    "\n",
    "tag_map = defaultdict(lambda : \"n\")  # by default, assume nouns\n",
    "tag_map['J'] = \"a\"  # adjectives\n",
    "tag_map['V'] = \"v\"  # verbs\n",
    "tag_map['R'] = \"r\"  # adverbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually convert the tags from one type to another, the following steps need to happen:\n",
    "\n",
    "1. Tag each token using `pos_tag`\n",
    "2. For each token, look in `tag_dict` for the tag that matches the *first letter* of the tag (all the different types of adverb start with \"R\" for the PoS tags, so this forces all those tags into one; this holds true for other types) \n",
    "3. If the first letter doesn't match, treat the word as a noun (this is a fallback to past behaviour - no matter what happens, our lemmatiser is as good as the default)\n",
    "4. Return the new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the pos tags for a set of tokens, and return the tokens in a way the\n",
    "# lemmatizer can interpret\n",
    "def get_wordnet_tags(tokens):\n",
    "    \"\"\"Returns WordNet pos_tags for a set of tokens\"\"\"\n",
    "    \n",
    "    # Tag tokens with pos_tagger\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Convert each tag to a version wordnet can understand\n",
    "    tagged_tokens = [(token[0], tag_map[token[1][0]]) for token in tagged_tokens]\n",
    "    \n",
    "    return tagged_tokens\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'n'),\n",
       " ('runs', 'v'),\n",
       " ('faster', 'r'),\n",
       " ('through', 'n'),\n",
       " ('the', 'n'),\n",
       " ('shallows', 'n'),\n",
       " ('to', 'n'),\n",
       " ('the', 'n'),\n",
       " ('sandiest', 'a'),\n",
       " ('banks', 'n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_tags(example_a_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is now represented as a `tuple` of the word itself and a tag that the lemmatiser can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex lemmatisation\n",
    "\n",
    "Once you have tagged tokens, lemmatising the words more effectively involves only a slight alteration to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'run', 'faster', 'through', 'the', 'shallow', 'to', 'the', 'sandy', 'bank']\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatisation to each word/token in example_a, using pos tags\n",
    "\n",
    "# Tag each token\n",
    "example_a_tagged = get_wordnet_tags(example_a_tokens)\n",
    "\n",
    "example_a_lemmas = [lemma.lemmatize(word=token[0], pos=token[1]) for token in example_a_tagged]\n",
    "\n",
    "print(example_a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'thank', 'the', 'woman', 'for', 'the', 'happy', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatisation to each word/token in example_a, using pos tags\n",
    "\n",
    "# Tag each token\n",
    "example_b_tagged = get_wordnet_tags(example_b_tokens)\n",
    "\n",
    "example_b_lemmas = [lemma.lemmatize(word=token[0], pos=token[1]) for token in example_b_tagged]\n",
    "\n",
    "print(example_b_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still not perfect (\"faster\" has not been simplified), but text analysis deals with such complex and messy data that very little ever is.\n",
    "\n",
    "There is a noticeable improvement though - \"thank\" and \"happier\" have both been correctly reduced to their lemma forms, making our data more consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation within a column/dataframe\n",
    "\n",
    "So far, we have lemmatised only with individual strings. The below code demonstrates how to apply these concepts to a column in a dataframe.\n",
    "\n",
    "For this particular example, we're using Shakespeare's Sonnet 116, because who says data scientists can't appreciate culture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=[\"line_number\", \"text\"], data=[[1, \"Let me not to the marriage of true minds\"],\n",
    "                                                         [2, \"Admit impediments. Love is not love\"],\n",
    "                                                         [3, \"Which alters when its alteration finds,\"],\n",
    "                                                         [4, \"Or bends with the remover to remove.\"],\n",
    "                                                         [5, \"O no! it is an ever-fixed mark\"],\n",
    "                                                         [6, \"That looks on tempests and is never shaken;\"],\n",
    "                                                         [7, \"It is the star to every wand'ring bark,\"],\n",
    "                                                         [8, \"Whose worth's unknown, although his height be taken.\"],\n",
    "                                                         [9, \"Love's not Time's fool, though rosy lips and cheeks\"],\n",
    "                                                         [10, \"Within his bending sickle's compass come;\"],\n",
    "                                                         [11, \"Love alters not with his brief hours and weeks,\"],\n",
    "                                                         [12, \"But bears it out even to the edge of doom.\"],\n",
    "                                                         [13, \"If this be error and upon me prov'd,\"],\n",
    "                                                         [14, \"I never writ, nor no man ever lov'd.\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text a little\n",
    "\n",
    "# Manage poetic conventions\n",
    "df[\"text\"].replace(\"'r\", \"er\", regex=True, inplace=True)\n",
    "df[\"text\"].replace(\"'d\", \"ed\", regex=True, inplace=True)\n",
    "\n",
    "# Remove punctuation\n",
    "df[\"text\"].replace(\"-\", \" \", regex=True, inplace=True)\n",
    "df[\"text\"].replace(\"[^\\w\\s]\", \"\", regex=True, inplace=True)\n",
    "\n",
    "# Case-fold for consistency\n",
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              let me not to the marriage of true minds\n",
       "1                    admit impediments love is not love\n",
       "2                which alters when its alteration finds\n",
       "3                   or bends with the remover to remove\n",
       "4                         o no it is an ever fixed mark\n",
       "5            that looks on tempests and is never shaken\n",
       "6                it is the star to every wandering bark\n",
       "7     whose worths unknown although his height be taken\n",
       "8      loves not times fool though rosy lips and cheeks\n",
       "9               within his bending sickles compass come\n",
       "10       love alters not with his brief hours and weeks\n",
       "11            but bears it out even to the edge of doom\n",
       "12                  if this be error and upon me proved\n",
       "13                   i never writ nor no man ever loved\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the cleaned text\n",
    "\n",
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the text\n",
    "\n",
    "df[\"tokens\"] = df[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the tokens\n",
    "\n",
    "df[\"tagged\"] = df[\"tokens\"].apply(get_wordnet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatise the tagged tokens\n",
    "\n",
    "df[\"lemmas\"] = df[\"tagged\"].apply(lambda tokens: [lemma.lemmatize(word=token[0], pos=token[1])\n",
    "                                                   for token in tokens]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [let, me, not, to, the, marriage, of, true, mind]\n",
       "1              [admit, impediment, love, be, not, love]\n",
       "2            [which, alter, when, it, alteration, find]\n",
       "3            [or, bend, with, the, remover, to, remove]\n",
       "4                  [o, no, it, be, an, ever, fix, mark]\n",
       "5      [that, look, on, tempest, and, be, never, shake]\n",
       "6       [it, be, the, star, to, every, wandering, bark]\n",
       "7     [whose, worth, unknown, although, his, height,...\n",
       "8     [love, not, time, fool, though, rosy, lip, and...\n",
       "9         [within, his, bending, sickle, compass, come]\n",
       "10    [love, alters, not, with, his, brief, hour, an...\n",
       "11    [but, bear, it, out, even, to, the, edge, of, ...\n",
       "12          [if, this, be, error, and, upon, me, prove]\n",
       "13           [i, never, writ, nor, no, man, ever, love]\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the lemmas\n",
    "\n",
    "df[\"lemmas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the lemmatised text back together\n",
    "\n",
    "df[\"processed_text\"] = df[\"lemmas\"].apply(lambda lemmas: \" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             let me not to the marriage of true mind\n",
       "1                   admit impediment love be not love\n",
       "2                 which alter when it alteration find\n",
       "3                  or bend with the remover to remove\n",
       "4                         o no it be an ever fix mark\n",
       "5             that look on tempest and be never shake\n",
       "6              it be the star to every wandering bark\n",
       "7     whose worth unknown although his height be take\n",
       "8        love not time fool though rosy lip and cheek\n",
       "9              within his bending sickle compass come\n",
       "10       love alters not with his brief hour and week\n",
       "11           but bear it out even to the edge of doom\n",
       "12                 if this be error and upon me prove\n",
       "13                  i never writ nor no man ever love\n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the results\n",
    "\n",
    "df[\"processed_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code breaks the lemmatisation process down into several steps; this process could be shortened significantly - even into a single line, if you really wanted, but we took the slow route to show each transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
