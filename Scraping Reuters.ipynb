{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reuters\n",
    "\n",
    "This notebook goes through the process of scraping articles from Reuters for a particular search term.\n",
    "\n",
    "This was an interesting challenge because the Reuters website is awkward, with dynamic content and weird database pulls.\n",
    "\n",
    "The following solution is **extremely** hacky, and probably not the best way to do anything. It will stop working immediately if Reuters ever changes minor details about their site, and there is a chance that using it will get your IP banned from Reuters (they definitely don't want you to do this).\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "I did this as an intellectual puzzle; I'm not at all suggesting that anyone should scrape Reuters.\n",
    "\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # Parse html\n",
    "import requests  # Make HTTP requests\n",
    "from selenium import webdriver  # Scraping browser for dealing with dynamic content\n",
    "from time import sleep  # Pause every so often to avoid Reuters noticing\n",
    "import pandas as pd  # Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the initial URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search terms: romance\n",
      "https://uk.reuters.com/search/news?blob=romance\n"
     ]
    }
   ],
   "source": [
    "search_terms = input(\"Enter search terms: \").replace(\" \", \"+\")\n",
    "\n",
    "url = f\"https://uk.reuters.com/search/news?blob={search_terms}\"\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the article links\n",
    "\n",
    "This is the hacky bit.\n",
    "\n",
    "We use [Selenium](https://pypi.org/project/selenium/) to automate clicking through the site to load sufficient links, then grab all the links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages of results should be loaded? 5\n"
     ]
    }
   ],
   "source": [
    "page_num = int(input(\"How many pages of results should be loaded? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 links.\n"
     ]
    }
   ],
   "source": [
    "# Create an automated browser\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Direct the browser to the correct URL; this loads 10 results\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Click the \"accept cookies\" button\n",
    "\n",
    "cookie_accept = driver.find_element_by_id(\"_evidon-banner-acceptbutton\")\n",
    "\n",
    "cookie_accept.click()\n",
    "\n",
    "# For a set number of times\n",
    "for i in range(page_num):\n",
    "    next_button = driver.find_element_by_class_name(\"search-result-more-txt\")\n",
    "    \n",
    "    # Break out if there are no more results\n",
    "    if next_button.text == \"NO MORE RESULTS\":\n",
    "        print(\"Reached end of results.\")\n",
    "        break\n",
    "        \n",
    "    # Click the \"more results\" button\n",
    "    next_button.click()\n",
    "    \n",
    "    # Pause to be polite\n",
    "    sleep(2)\n",
    "\n",
    "# Grab all links on the page\n",
    "\n",
    "links = driver.find_elements_by_tag_name('a')\n",
    "\n",
    "# Remove duplicates and extract just the links themselves\n",
    "\n",
    "links = list(set([link.get_attribute('href') for link in links]))\n",
    "\n",
    "# Only keep links with \"article\" in them somewhere\n",
    "\n",
    "links = [link for link in links if \"article\" in link]\n",
    "    \n",
    "# Close the Selenium driver for neatness\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Check the number of links returned - ideally would be equal to page_num - 1 / 10\n",
    "\n",
    "print(f\"Found {len(links)} links.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract details from a single article link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reuters_article_details(url):\n",
    "    \"\"\"\n",
    "    Access an article from the Reuters site and return the article details\n",
    "    \n",
    "    Parameters:\n",
    "        - url (str): the URL of a Reuters article\n",
    "\n",
    "    Returns:\n",
    "        - details (dict): a dictionary of article details\n",
    "            - url\n",
    "            - title\n",
    "            - attribution\n",
    "            - date\n",
    "            - category\n",
    "            - content\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a request for the page\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Construct a soup object from the page contents\n",
    "    \n",
    "    soup = BeautifulSoup(page.content)\n",
    "    \n",
    "    # Build the dictionary by pulling the text of different elements from the page\n",
    "    \n",
    "    details = {\"url\": url}\n",
    "    \n",
    "    # Hacky error handling\n",
    "    \n",
    "    title = soup.find(\"h1\", class_=\"ArticleHeader_headline\")\n",
    "    details[\"title\"] = title.text if title else None\n",
    "    \n",
    "    attribution = soup.find(\"p\", class_=\"Attribution_content\")\n",
    "    details[\"attribution\"] = attribution.text if attribution else None\n",
    "    \n",
    "    date = soup.find(\"div\", class_=\"ArticleHeader_date\")\n",
    "    details[\"date\"] = date.text if date else None\n",
    "\n",
    "    category = soup.find(\"div\", class_=\"ArticleHeader_channel\")\n",
    "    category_link = category.find(\"a\") if category else None\n",
    "    details[\"category\"] = category_link.text if category_link else None\n",
    "    \n",
    "    # Extract the paragraphs of content (immediate <p> children of the body) and join them together\n",
    "    \n",
    "    body = soup.find(\"div\", class_=\"StandardArticleBody_body\")\n",
    "    paragraphs = body.findChildren(\"p\" , recursive=False) if body else []\n",
    "    \n",
    "    # Join the text together\n",
    "    \n",
    "    text = \"\\n\".join([paragraph.text for paragraph in paragraphs])\n",
    "    \n",
    "    # Store the text in a dictionary\n",
    "    \n",
    "    details[\"content\"] = text\n",
    "    \n",
    "    # Return the details dict\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract details for all articles\n",
    "\n",
    "Now that we have a function to extract one articles' details, we can simply apply it to each article in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles scraped: 61/61\r"
     ]
    }
   ],
   "source": [
    "# Make a holder for article details\n",
    "\n",
    "article_holder = []\n",
    "\n",
    "# Loop through the links and extract article details for each one\n",
    "\n",
    "for link in links:\n",
    "    details = get_reuters_article_details(link)\n",
    "    article_holder.append(details)\n",
    "    \n",
    "    # Wait to avoid detection\n",
    "    sleep(1)\n",
    "    \n",
    "    # Output the article number \n",
    "    print(f\"Articles scraped: {links.index(link) + 1}/{len(links)}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the details in a dataframe\n",
    "\n",
    "Now that we have a list of dicts, we can transform it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>attribution</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://uk.reuters.com/article/idUKKBN0DI0S220...</td>\n",
       "      <td>News Corp to buy Torstar's romance publisher H...</td>\n",
       "      <td>Reporting by Euan Rocha in Toronto and Ashutos...</td>\n",
       "      <td>May 2, 2014 /  12:31 PM / 6 years ago</td>\n",
       "      <td>Media Industry News</td>\n",
       "      <td>TORONTO, May 2 (Reuters) - News Corp NWSA.O sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://uk.reuters.com/article/idUKTRE81513420...</td>\n",
       "      <td>Finding out what went wrong with failed romance</td>\n",
       "      <td>Reporting by Natasha Baker; editing by Patrici...</td>\n",
       "      <td>February 6, 2012 /  2:26 PM / 8 years ago</td>\n",
       "      <td>Technology News</td>\n",
       "      <td>SAN FRANCISCO (Reuters) - With Valentine’s Day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://uk.reuters.com/article/idUKKCN1R111M</td>\n",
       "      <td>Open that door? Netflix explores choose-your-o...</td>\n",
       "      <td>Reporting by Lisa Richwine; Editing by Darren ...</td>\n",
       "      <td>March 20, 2019 /  10:04 AM / a year ago</td>\n",
       "      <td>Business News</td>\n",
       "      <td>LOS ANGELES (Reuters) - A Netflix Inc experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://uk.reuters.com/article/idUKL1N2150ZR</td>\n",
       "      <td>Open that door? Netflix explores choose-your-o...</td>\n",
       "      <td>Reporting by Lisa Richwine; Editing by Darren ...</td>\n",
       "      <td>March 20, 2019 /  10:04 AM / a year ago</td>\n",
       "      <td>Business News</td>\n",
       "      <td>LOS ANGELES (Reuters) - A Netflix Inc experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://uk.reuters.com/article/idUKKCN1QA2M9</td>\n",
       "      <td>Prada contrasts two sides of romance at Milan ...</td>\n",
       "      <td>Reporting by Claudia Cristoferi and Marie-Loui...</td>\n",
       "      <td>February 21, 2019 /  7:25 PM / a year ago</td>\n",
       "      <td>Entertainment News</td>\n",
       "      <td>MILAN (Reuters) - Italian luxury label Prada a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://uk.reuters.com/article/idUKKBN0DI0S220...   \n",
       "1  https://uk.reuters.com/article/idUKTRE81513420...   \n",
       "2       https://uk.reuters.com/article/idUKKCN1R111M   \n",
       "3       https://uk.reuters.com/article/idUKL1N2150ZR   \n",
       "4       https://uk.reuters.com/article/idUKKCN1QA2M9   \n",
       "\n",
       "                                               title  \\\n",
       "0  News Corp to buy Torstar's romance publisher H...   \n",
       "1    Finding out what went wrong with failed romance   \n",
       "2  Open that door? Netflix explores choose-your-o...   \n",
       "3  Open that door? Netflix explores choose-your-o...   \n",
       "4  Prada contrasts two sides of romance at Milan ...   \n",
       "\n",
       "                                         attribution  \\\n",
       "0  Reporting by Euan Rocha in Toronto and Ashutos...   \n",
       "1  Reporting by Natasha Baker; editing by Patrici...   \n",
       "2  Reporting by Lisa Richwine; Editing by Darren ...   \n",
       "3  Reporting by Lisa Richwine; Editing by Darren ...   \n",
       "4  Reporting by Claudia Cristoferi and Marie-Loui...   \n",
       "\n",
       "                                        date             category  \\\n",
       "0      May 2, 2014 /  12:31 PM / 6 years ago  Media Industry News   \n",
       "1  February 6, 2012 /  2:26 PM / 8 years ago      Technology News   \n",
       "2    March 20, 2019 /  10:04 AM / a year ago        Business News   \n",
       "3    March 20, 2019 /  10:04 AM / a year ago        Business News   \n",
       "4  February 21, 2019 /  7:25 PM / a year ago   Entertainment News   \n",
       "\n",
       "                                             content  \n",
       "0  TORONTO, May 2 (Reuters) - News Corp NWSA.O sa...  \n",
       "1  SAN FRANCISCO (Reuters) - With Valentine’s Day...  \n",
       "2  LOS ANGELES (Reuters) - A Netflix Inc experime...  \n",
       "3  LOS ANGELES (Reuters) - A Netflix Inc experime...  \n",
       "4  MILAN (Reuters) - Italian luxury label Prada a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "\n",
    "reuters_df = pd.DataFrame(article_holder)\n",
    "\n",
    "# View the dataframe\n",
    "\n",
    "reuters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and parsing date/attribution values is left as an exercise for the reader."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit8a2add6b42224167be4696eeaa3c58e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
